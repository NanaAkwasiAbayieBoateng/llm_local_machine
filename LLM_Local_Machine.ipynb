{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Large language models (LLMs) have revolutionized natural language processing by enabling advanced applications such as text generation, sentiment analysis, and conversational agents. Running these models on a local machine allows developers and enthusiasts to leverage their power without relying on cloud services, promoting greater accessibility and experimentation. However, successfully implementing LLMs locally necessitates a thorough understanding of the associated hardware and software requirements, as well as optimal setup procedures. To effectively utilize LLMs on a personal computer, users must meet specific hardware specifications, including system compatibility, adequate memory, and powerful GPUs capable of handling extensive computational tasks. For instance, models with billions of parameters demand substantial RAM and GPU resources, making it essential for users to evaluate their system capabilities before installation. Additionally, software frameworks like TensorFlow and PyTorch are integral to managing LLMs, necessitating a well-configured environment to ensure seamless operation. Setting up a local environment involves several critical steps, including creating virtual environments, installing necessary libraries, and managing dependencies. Users must also navigate potential challenges such as installation errors and performance optimization, particularly regarding memory management and GPU acceleration. Addressing these concerns is crucial for maximizing the performance and responsiveness of LLMs on personal machines, allowing users to tailor models to specific tasks effectively. While local implementation of LLMs is increasingly accessible, it is not without its complexities. Notable challenges include hardware limitations, compatibility issues, and the need for efficient troubleshooting strategies. Understanding these intricacies is vital for users aiming to harness the full potential of LLMs, ensuring they can execute advanced AI applications locally while mitigating common pitfalls associated with setup and performance optimization.\n",
    "\n",
    "### Requirements\n",
    "To effectively run large language models (LLMs) on a local machine, it is crucial to understand and meet specific hardware and software requirements. This ensures optimal performance and a smooth user experience while leveraging the capabilities of LLMs.\n",
    "\n",
    "#### Hardware Requirements\n",
    "#### System Compatibility\n",
    "\n",
    "Before diving into the installation of an LLM, you need to ensure that your operating system is compatible. For MacOS users, versions 11 Big Sur or later are recommended, while Linux users should have Ubuntu 18.04 or later. Windows users can utilize Windows Subsystem for Linux 2 (WSL2) to run LLMs effectively on their machines[[1]](#1).\n",
    "\n",
    "\n",
    "#### Memory Specifications\n",
    "Memory plays a vital role in running LLMs. For smaller models, such as those with around 3 billion parameters, a minimum of 8GB of RAM is necessary. As the model size increases, so do the memory requirements; 7B models require 16GB of RAM, while 13B models benefit from at least 32GB[[1]](#1)[[2]](#2). Ensuring adequate memory is essential for smooth operation and responsiveness of the models.\n",
    "\n",
    "**GPU Requirements**\n",
    "\n",
    "The GPU is arguably the most critical component when it comes to running LLMs. They are responsible for handling the complex matrix multiplications and parallel processing tasks required for both training and inference. Recommended GPUs for LLM tasks include the NVIDIA A100 Tensor Core GPU, which has 40GB or more of VRAM, and consumer-grade options like the NVIDIA RTX 4090/3090, which feature 24GB of VRAM [[2]](#2)[[3]](#3). Additionally, for larger models, having a GPU with at least 10GB of VRAM is advisable to ensure efficient processing[[4]](#4).\n",
    "\n",
    "#### Disk Space and Storage\n",
    "Adequate disk space is essential for installing the necessary software and storing model files. While the specific requirements can vary based on the models being used, ensuring you have sufficient space to accommodate multiple models and datasets is vital.\n",
    "\n",
    "#### Software Requirements\n",
    "Operating System and Software Support\n",
    "It is important to have a system that supports the necessary software frameworks and libraries for LLMs, such as TensorFlow, PyTorch, Hugging Face Transformers, and DeepSpeed. Most AI developers prefer Linux-based systems due to better support for these tools[[2]](#2)[[3]](#3).\n",
    "\n",
    "#### Networking and Connectivity\n",
    "For larger deployments or setups involving multiple machines, a robust networking setup is essential. A 10 Gigabit Ethernet connection is recommended for transferring large model checkpoints or datasets efficiently. If relying on wireless connectivity, WiFi 6 is advisable for improved throughput and reduced latency[[2]](#2)[[3]](#3).\n",
    "\n",
    "### Setting Up the Environment\n",
    "To successfully utilize large language models (LLMs) on a local machine, it's crucial to set up your development environment correctly. This process involves creating virtual environments, installing necessary dependencies, and ensuring that your system meets specific requirements.\n",
    "\n",
    "#### Downloading Large Language Models\n",
    "Downloading large language models (LLMs) for local use is a crucial step for individuals and developers aiming to leverage these powerful tools for various applications. This section outlines the key considerations and steps involved in obtaining LLMs.\n",
    "\n",
    "#### Accessing Model Repositories\n",
    "The primary source for downloading LLMs is online repositories, with Hugging Face being one of the most popular platforms. Users can search for models based on their specific needs and select from a wide range of options, including models tuned for conversational tasks, such as instruct-tuned variants designed to handle interactive dialogues effectively[[5]](#5)[[6]](#6).\n",
    "\n",
    "#### Choosing the Right Model\n",
    "When selecting a model, it is essential to consider its architecture and intended use case. Many models come pre-trained and can be fine-tuned for specific tasks or applications. For example, models like GPT-3 and BERT are widely recognized for their versatility in natural language processing tasks, ranging from text generation to sentiment analysis[[7]](#7). Additionally, it's important to pay attention to model specifications, such as the number of parameters and quantization options, which can affect performance and resource requirements[[8]](#8)[[5]](#5).\n",
    "\n",
    "\n",
    "#### Downloading and Managing Models\n",
    "Once a suitable model is identified, downloading it is typically straightforward. Many LLM frameworks provide built-in mechanisms to handle model downloads seamlessly, often defaulting to versions optimized for local usage, such as 4-bit quantized models[[6]](#6). For a manual approach, users can directly visit the Hugging Face Model Hub, where they can select the desired model repository and download the model files. The process generally involves copying the model name and specific file needed for implementation[[5]](#5).\n",
    "\n",
    "#### Setting Up the Environment\n",
    "After downloading, users will need to set up their local environment to run the model. This often includes installing required libraries and dependencies specific to the framework being used, such as TensorFlow or PyTorch. Users should familiarize themselves with the framework's documentation to ensure proper configuration and optimization for their hardware setup[[7]](#7)[[9]](#9).\n",
    "\n",
    "### Running Large Language Models Locally\n",
    "Running large language models (LLMs) locally has become increasingly accessible, allowing users to leverage powerful AI tools directly on their personal computers. With advancements in model efficiency and the availability of various frameworks, users can now experiment with models like GPT-3, LLaMA, and others without relying solely on cloud-based solutions.\n",
    "\n",
    "#### Getting Started with Local LLMs\n",
    "Before diving into running LLMs locally, it's essential to understand the necessary prerequisites. Users should familiarize themselves with concepts such as transformer architecture, pre-training, and fine-tuning, which are critical for effectively utilizing these models[[7]](#7). For initial experimentation, it is advisable to start with smaller models like GPT-2, which are easier to work with and require less computational power[[7]](#7).\n",
    "\n",
    "\n",
    "#### Installation and Setup\n",
    "\n",
    "To run an LLM locally, users need to install the relevant libraries and dependencies. Popular libraries for this purpose include Hugging Face's Transformers and Llama CPP. After ensuring that the necessary software is installed, users can download a model using the model browser provided by these frameworks. This tool integrates with platforms like Hugging Face to facilitate file management[[9]](#9)[[5]](#5).\n",
    "\n",
    "#### Model Loading and Configuration\n",
    "\n",
    "Once a model is downloaded, users must configure it according to their system capabilities. This involves specifying parameters such as context length and the number of CPU threads to utilize. For instance, one might set the context length to 4096 tokens and allocate four CPU threads for processing[[5]](#5). After configuring these settings, the model can be instantiated and tested for basic functionality.\n",
    "\n",
    "#### Fine-Tuning Models\n",
    "\n",
    "Fine-tuning is a crucial step in adapting pre-trained models to specific tasks or datasets. If a user possesses a labeled dataset tailored for a particular application, such as sentiment analysis or question answering, they can fine-tune the model to enhance its performance on that task. This process involves using training scripts and defining appropriate training parameters within the chosen framework[[10]](#10). Detailed documentation is typically available to guide users through this process effectively.\n",
    "\n",
    "#### Challenges and Considerations\n",
    "While running LLMs locally is more feasible than ever, users should be aware of potential challenges. The computational demands of larger models can strain consumer-grade hardware, making it essential to choose the right model based on system specifications[[11]](#11). Additionally, experimenting with local models may require troubleshooting and adjustments to ensure compatibility and performance stability.\n",
    "\n",
    "### Performance Optimization\n",
    "Optimizing the performance of large language models (LLMs) when running locally is crucial for achieving efficient and effective results. Several strategies can be employed to maximize the capabilities of LLMs like Llama 2 and Llama 3.1, ensuring that they run smoothly while utilizing system resources effectively.\n",
    "\n",
    "#### GPU Acceleration\n",
    "One of the most significant enhancements in performance can be achieved through the use of GPU acceleration. Modern GPUs, especially high-end models such as the NVIDIA GeForce RTX series, are designed to handle computationally intensive tasks with greater efficiency compared to CPUs. To leverage this, users should ensure that their systems are configured to utilize GPU capabilities by installing the necessary drivers and libraries, such as CUDA for NVIDIA GPUs or ROCm for AMD GPUs[[12]](#12). Utilizing GPUs can drastically reduce processing time, particularly when managing large inputs or executing multiple tasks concurrently[[12]](#12)[[13]](#13).\n",
    "\n",
    "#### Batching Techniques\n",
    "Batching is another effective method for optimizing throughput and latency in model inference. By processing multiple input sequences simultaneously, batching maximizes the efficiency of matrix operations, enabling the model to handle more tokens per unit time. However, selecting the appropriate batch size involves trade-offs; smaller batch sizes can reduce latency but may lower overall throughput, while larger batch sizes can enhance throughput at the expense of increased latency[[14]](#14). It is essential to tailor the batch size to the specific use case requirements, balancing the need for quick responses against the desire for maximum processing efficiency[[14]](#14).\n",
    "\n",
    "\n",
    "#### Memory Management\n",
    "Efficient memory usage plays a crucial role in optimizing performance. Implementing batch processing not only reduces redundant operations but also optimizes resource utilization. Careful alignment of batch sizes with system memory capacity can help prevent crashes and enhance performance. Additionally, monitoring system resource usage with tools like NVIDIA’s nvidia-smi or Linux’s htop can help identify bottlenecks in memory, GPU, or CPU usage, allowing users to make necessary adjustments[[12]](#12)[[15]](#15).\n",
    "\n",
    "\n",
    "#### Regular Updates and Dependency Management\n",
    "Keeping software dependencies up to date is vital for maintaining optimal performance. Regularly updating Python libraries, CUDA drivers, and the Llama repository can lead to significant performance enhancements, as updates often include bug fixes and new features that improve functionality[[12]](#12).\n",
    "\n",
    "#### Fine-Tuning Model Parameters\n",
    "Fine-tuning model parameters can also yield considerable improvements in performance. Parameter-efficient tuning methods, such as Low-Rank Adaptation (LoRA) and Prefix Tuning, allow for fine-tuning only a subset of model parameters, which reduces computational and memory overhead[[16]](#16). These techniques have been shown to achieve performance levels comparable to full fine-tuning while being significantly less resource-intensive.\n",
    "\n",
    "#### Structural Optimization\n",
    "Structural optimizations, such as the implementation of FlashAttention and PagedAttention, can enhance computational speed by minimizing memory access operations during forward propagation. These optimizations utilize a chunked computation approach, which significantly boosts performance by reducing the number of accesses to high bandwidth memory (HBM) and speeding up the overall inference process[[17]](#17)[[16]](#16). By employing these strategies, users can significantly enhance the performance of large language models on local machines, making them faster, more responsive, and better suited for a variety of applications.\n",
    "\n",
    "### Troubleshooting Common Issues\n",
    "When working with large language models (LLMs) on a local machine, users may encounter various issues that can hinder their experience. Below are some common problems and recommended solutions to address them effectively.\n",
    "\n",
    "#### Node-Level Replacement Issues\n",
    "One common challenge arises when node-level replacement is needed during model execution. If a node fails, collectives may hang instead of throwing an error. To mitigate this, it is essential to set appropriate timeouts on collectives to ensure they throw an error when necessary. Additionally, implementing a monitoring client that tracks CloudWatch logs and metrics can help identify abnormal patterns, such as halted logging or zero GPU usage, which indicate job hangs or convergence issues. This setup allows for prompt remediation by enabling automatic job stop/retry actions[[18]](#18).\n",
    "\n",
    "#### Context-Memory Conflicts\n",
    "Context-memory conflicts can occur when external contexts contradict the internal knowledge of the LLM. To address these conflicts, fine-tuning the model on counterfactual contexts can help prioritize external information. Using specialized prompts reinforces adherence to context, while decoding techniques can amplify context probabilities. Additionally, pre-training on diverse contexts across documents aids in reducing the incidence of such conflicts[[19]](#19).\n",
    "\n",
    "#### Installation Issues\n",
    "Users may experience difficulties when installing packages like PyTorch on their machines. It is advisable to follow official installation instructions to avoid errors. For instance, for a standard installation on Windows using Anaconda, one might use commands like conda install pytorch torchvision cpuonly -c pytorch for CPU versions, or include the CUDA toolkit for GPU support[[20]](#20)[[21]](#21). If issues persist, verifying the installation with a simple PyTorch code snippet can help confirm that everything is set up correctly[[21]](#21).\n",
    "\n",
    "#### Model Downloading Errors\n",
    "When downloading models, particularly for machines with varying GPU capabilities, users may encounter CudaOOM errors if the target machine has insufficient resources. It is crucial to ensure compatibility between the downloading and executing machines. For better management of LLMs, tools like hugging face-cli or hf_hub_download can provide a more reliable method to fetch models[[22]](#22)[[23]](#23).\n",
    "\n",
    "#### Post-Installation Setup\n",
    "After installing an LLM application like Jan.AI or Ollama, users should ensure their system's environment is correctly set up. For instance, ensuring that the video card (preferably NVIDIA) is recognized is critical for optimal performance. If a suitable GPU is unavailable, the model may resort to CPU usage, significantly slowing down processing times[[24]](#24). By addressing these common issues with proactive measures, users can enhance their experience with large language models on local machines, ensuring smoother operations and better outcomes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"References \"></a>\n",
    "### References\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"1\">[1]</a> \n",
    "https://aitoolmall.com/news/running-local-llms-made-easy-with-ollama-ai/\n",
    "\n",
    "<a id=\"2\">[2]</a> \n",
    "https://www.linkedin.com/pulse/interacting-metas-latest-llama-32-llms-locally-using-ollama-stafford-fhduc/\n",
    "\n",
    "<a id=\"3\">[3]</a> \n",
    "https://www.pcmag.com/how-to/how-to-run-your-own-chatgpt-like-llm-for-free-and-in-private\n",
    "\n",
    "<a id=\"4\">[4]</a> \n",
    "https://www.hardware-corner.net/llm-database/LLaMA/\n",
    "\n",
    "<a id=\"5\">[5]</a> \n",
    "https://www.restack.io/p/transformers-answer-hugging-face-install-cat-ai\n",
    "\n",
    "<a id=\"6\">[6]</a> \n",
    "https://scoris.medium.com/a-beginners-journey-into-ai-guide-to-running-large-language-models-locally-4a9c44fa8fef\n",
    "\n",
    "<a id=\"7\">[7]</a> \n",
    "https://medium.com/@dinakarchennupati777/the-recommended-way-to-setup-pytorch-environment-on-your-local-machine-b90ec1eef5dd\n",
    "\n",
    "<a id=\"8\">[8]</a> \n",
    "https://www.tomshardware.com/news/running-your-own-chatbot-on-a-single-gpu\n",
    "\n",
    "<a id=\"9\">[9]</a> \n",
    "https://www.pcguide.com/apps/can-chatgpt-run-locally/\n",
    "\n",
    "\n",
    "<a id=\"10\">[10]</a> \n",
    "https://www.almabetter.com/bytes/articles/install-pytorch\n",
    "\n",
    "\n",
    "<a id=\"11\">[11]</a> \n",
    "https://blog.ahmadwkhan.com/running-open-source-llm-llama3-locally-using-ollama-and-langchain\n",
    "\n",
    "<a id=\"12\">[12]</a> \n",
    "https://medium.com/predict/a-simple-comprehensive-guide-to-running-large-language-models-locally-on-cpu-and-or-gpu-using-c0c2a8483eee\n",
    "\n",
    "\n",
    "<a id=\"13\">[13]</a> \n",
    "https://www.edtech247.com/blog/local-llm/\n",
    "\n",
    "<a id=\"14\">[14]</a> \n",
    "https://medium.com/@siamsoftlab/getting-started-with-large-language-models-945b6d943f01\n",
    "\n",
    "<a id=\"15\">[15]</a> \n",
    "https://raptorhacker.medium.com/20-minutes-for-beginners-to-deploy-large-language-model-locally-3ab66d71ef1c\n",
    "\n",
    "<a id=\"16\">[16]</a> \n",
    "**seven ways of running llm locally**\n",
    "https://kleiber.me/blog/2024/01/07/seven-ways-running-llm-locally/\n",
    "\n",
    "\n",
    "<a id=\"17\">[17]</a> \n",
    "https://mljourney.com/how-to-use-hugging-face-step-by-step-guide/\n",
    "\n",
    "\n",
    "<a id=\"18\">[18]</a> \n",
    "https://www.toolify.ai/ai-news/running-large-language-models-on-your-computer-a-guide-to-koboldcpp-and-autogptq-1217326\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"19\">[19]</a> \n",
    "https://mljourney.com/how-to-run-llama-2-locally-a-step-by-step-guide/\n",
    "\n",
    "\n",
    "<a id=\"20\">[20]</a> \n",
    "https://www.hardware-corner.net/guides/hardware-for-120b-llm/\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"21\">[21]</a> \n",
    "https://medium.com/@yvan.fafchamps/how-to-benchmark-and-optimize-llm-inference-performance-for-data-scientists-1dbacdc7412a\n",
    "\n",
    "\n",
    "<a id=\"22\">[22]</a> \n",
    "https://medium.com/@mne/how-to-choose-the-ideal-large-language-model-for-local-inference-c2b25931205\n",
    "\n",
    "\n",
    "<a id=\"23\">[23]</a> \n",
    "https://arxiv.org/html/2401.02038v2\n",
    "\n",
    "\n",
    "<a id=\"24\">[24]</a> \n",
    "https://developers.googleblog.com/en/large-language-models-on-device-with-mediapipe-and-tensorflow-lite/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Virtual Environment\n",
    "\n",
    "You can install it with pip:\n",
    "\n",
    "- `conda create --name llm_local`\n",
    "\n",
    "list the available virtual environments:\n",
    "\n",
    "- `conda env list`\n",
    "\n",
    "Activate the virtual environment:\n",
    "\n",
    "`conda activate llm_local`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama\n",
    "\n",
    "Ollama is a self-hosted language model developed by OpenAI. It is designed to be easy to use, scalable, and secure. Ollama provides a simple REST API that allows users to interact with the model.You can use Ollama by  from the terminal by first installing an executable file available for download from this [link](https://ollama.com/download). You can also use the python library by `pip install ollama` to install.\n",
    "\n",
    "For example to run and chat with llama 3.3 run `ollama run llama3.3` in the terminal. This will download the model and afterwards you can chat with it. Different models are available from [here](https://ollama.com/search). To use a model , first pull it with for the first time, the command `ollama pull llama3.2:1b`  will pull the Llama 3.2 3B model. This command will install a 4-bit quantized version of the 3B model, which requires 2.0 GB of disk space and has an identical hash to the 3b-instruct-q4_K_M model. Afterwards you can download the model as  `ollama run llama3.2:1b` to start chatting with this model in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 7462734796d6... 100% ▕████████████████▏ 1.6 GB                         \n",
      "pulling e0a42594d802... 100% ▕████████████████▏  358 B                         \n",
      "pulling 097a36493f71... 100% ▕████████████████▏ 8.4 KB                         \n",
      "pulling 2490e7468436... 100% ▕████████████████▏   65 B                         \n",
      "pulling e18ad7af7efb... 100% ▕████████████████▏  487 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "#!ollama pull llama3.2:1b \n",
    "!ollama pull gemma2:2b\n",
    "#!ollama pull llama3.2:latest\n",
    "#!pip install streamlit ollama\n",
    "#!pip install langchain langchain-community langchain-core langchain-ollama streamlit watchdog "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ollama python library can also be used to interact with  llms as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 45.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Let\\'s break down perplexity and how it relates to large language models (LLMs):\\n\\n**What is Perplexity?**\\n\\nIn essence, perplexity is a measure of how well an LLM predicts the next word in a sequence.  Imagine you have a sentence: \"The quick brown fox jumps over the lazy dog.\" \\n\\n* **High Perplexity:** If your LLM has trouble predicting what the next word should be (like \"over\" or \"the\"), it indicates high perplexity. This suggests that the model doesn\\'t understand the context well and struggles to predict the most likely follow-up words.\\n* **Low Perplexity:**  If the model predicts the next word with high accuracy, it will have low perplexity. The model can grasp the relationship between words and make accurate predictions about what should come next.\\n\\n**Perplexity Explained**\\n\\nPerplexity is calculated based on the probabilities of all possible next words within a given context:\\n\\n1. **Training Data:** An LLM learns from massive amounts of text data (think books, articles, code) to predict what comes next in various contexts.\\n2. **Probability Distribution:** The model assigns probability scores to each word in the vocabulary. These probabilities reflect how likely that word is to follow another based on the context provided (for example, \"the,\" \"and,\" or \"fox\"). \\n3. **Calculating Perplexity:** Perplexity measures the average perplexity of all possible words at a particular step. The lower the perplexity, the more accurate the model\\'s prediction of words in a sequence.\\n\\n\\n**How Perplexity is Used**\\n\\nPerplexity is often used to:\\n\\n* **Evaluate LLMs:**  A lower perplexity score generally indicates a better-performing LLM. \\n* **Compare Models:** By comparing perplexity scores, you can see how different models handle various types of text and tasks (e.g., generating creative stories vs. summarizing factual articles). \\n* **Understand Model Performance:** Perplexity provides insights into the model\\'s level of confidence in its predictions. High perplexity suggests uncertainty or a need for further training.\\n\\n**Important Considerations**\\n\\n* **Task-specific:** Perplexity might vary based on the specific task you want your LLM to perform (e.g., translation, summarization, question answering).  \\n* **Context matters:** A sentence\\'s context plays a huge role in perplexity calculations – the model needs enough information about what precedes a word to make accurate predictions. \\n\\n\\n**In a Nutshell:**\\n\\nPerplexity is an indicator of how well an LLM \"understands\" text and predicts the next word, helping us evaluate its performance. The lower the perplexity score, the better!\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#pip install ollama\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "def generate_response(prompt,model):\n",
    "    try:\n",
    "        response: ChatResponse = chat(model, messages=[\n",
    "        {\n",
    "         'role': 'user',\n",
    "          'content': prompt},\n",
    "          ])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during response generation: {e}\")\n",
    "    \n",
    "    return response['message']['content']\n",
    "\n",
    "generate_response(prompt='what is perplexity in llm?',model='gemma2:2b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently or access fields directly from the response object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama with Streamlit and LangChain\n",
    "\n",
    "We will use Streamlit and LangChain to interact with the Llama 3.2 1B and 3B models using a chat application. The code below can be saved as `ollama_streamlit.py` then run `python -m streamlit run ollama_streamlit.py` in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_response(prompt,model):\n",
    "    try:\n",
    "        response: ChatResponse = chat(model, messages=[\n",
    "        {\n",
    "         'role': 'user',\n",
    "          'content': prompt},\n",
    "          ])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during response generation: {e}\")\n",
    "    \n",
    "    return response['message']['content']\n",
    "\n",
    "\n",
    "model_selected = st.selectbox('Select model', ['llama3.2:1b', 'gemma2:2b'])\n",
    "\n",
    "#generate_response(prompt='why is the sky blue ?')\n",
    "\n",
    "def main():\n",
    "    st.title(\"Ollama LLM App\")\n",
    "\n",
    "    # User input field\n",
    "    user_input = st.text_input(\"Enter your prompt:\")\n",
    "\n",
    "    # Button to trigger generation\n",
    "    if st.button(\"Generate\"):\n",
    "        if user_input:\n",
    "            #response = generate_response(user_input)\n",
    "            response = generate_response(prompt=user_input,model=model_selected)\n",
    "            st.write(\"Model Response:\")\n",
    "            st.write(response)\n",
    "        else:\n",
    "            st.warning(\"Please enter a prompt.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama can also be used via the api. You can first if the API is running via `http://localhost:11434/`. In windows powershell the commad to access the api for example is `Invoke-WebRequest -Uri http://localhost:11434/api/generate -Method Post -Body '{\"model\": \"llama3.2:1b\",\"prompt\": \"what is the biggest country in the world?\"}' -ContentType \"application/json\"`.The curl command in our linux/unix terminal to send a request to the API. curl http://localhost:11434/api/generate -d '{ \"model\": \"llama3.2:1b\", \"prompt\": \"What is the happiest place on earth?\" }'\n",
    "Another way to access the ollama api is through python. For example the python code below can be saved as a python script and run in the terminal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"llama3.2:1b\",\n",
    "    \"prompt\": \"what is the biggest country in the world\",\n",
    "    # Convert \"False\" to a boolean\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    response_text = response.text\n",
    "    data = json.loads(response_text)\n",
    "    actual_response = data[\"response\"]\n",
    "    print(actual_response)\n",
    "else:\n",
    "    print(\"Error: \", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama-Streamlit-LangChain-Chat-App\n",
    "# Streamlit app for chatting with Meta Llama 3.2 using Ollama and LangChain\n",
    "# Author: Gary A. Stafford\n",
    "# Date: 2024-09-26\n",
    "\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "\n",
    "import streamlit as st\n",
    "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama-Streamlit-LangChain-Chat-App\n",
    "# Streamlit app for chatting with Meta Llama 3.2 using Ollama and LangChain\n",
    "# Author: Gary A. Stafford\n",
    "# Date: 2024-09-26\n",
    "\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "\n",
    "import streamlit as st\n",
    "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Constants\n",
    "PAGE_TITLE = \"Llama 3.2 Chat\"\n",
    "PAGE_ICON = \"🦙\"\n",
    "SYSTEM_PROMPT = \"You are a friendly AI chatbot having a conversation with a human.\"\n",
    "DEFAULT_MODEL = \"llama3.2:latest\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def initialize_session_state() -> None:\n",
    "    defaults: Dict[str, Any] = {\n",
    "        \"model\": DEFAULT_MODEL,\n",
    "        \"input_tokens\": 0,\n",
    "        \"output_tokens\": 0,\n",
    "        \"total_tokens\": 0,\n",
    "        \"total_duration\": 0,\n",
    "        \"num_predict\": 2048,\n",
    "        \"seed\": 1,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.9,\n",
    "    }\n",
    "    for key, value in defaults.items():\n",
    "        if key not in st.session_state:\n",
    "            st.session_state[key] = value\n",
    "\n",
    "\n",
    "def create_sidebar() -> None:\n",
    "    with st.sidebar:\n",
    "        st.header(\"Inference Settings\")\n",
    "        st.session_state.system_prompt = st.text_area(\n",
    "            label=\"System\",\n",
    "            value=SYSTEM_PROMPT,\n",
    "            help=\"Sets the context in which to interact with the AI model. It typically includes rules, guidelines, or necessary information that help the model respond effectively.\",\n",
    "        )\n",
    "\n",
    "        st.session_state.model = st.selectbox(\n",
    "            \"Model\",\n",
    "            [\"llama3.2:1b\", \"llama3.2:latest\"],\n",
    "            index=1,\n",
    "            help=\"Select the model to use.\",\n",
    "        )\n",
    "        st.session_state.seed = st.slider(\n",
    "            \"Seed\",\n",
    "            min_value=1,\n",
    "            max_value=9007199254740991,\n",
    "            value=round(9007199254740991 / 2),\n",
    "            step=1,\n",
    "            help=\"Controls the randomness of how the model selects the next tokens during text generation.\",\n",
    "        )\n",
    "        st.session_state.temperature = st.slider(\n",
    "            \"Temperature\",\n",
    "            min_value=0.0,\n",
    "            max_value=1.0,\n",
    "            value=0.5,\n",
    "            step=0.01,\n",
    "            help=\"Sets an LLM's entropy. Low temperatures render outputs that are predictable and repetitive. Conversely, high temperatures encourage LLMs to produce more random, creative responses.\",\n",
    "        \n",
    "        )\n",
    "        st.session_state.top_p = st.slider(\n",
    "            \"Top P\",\n",
    "            min_value=0.0,\n",
    "            max_value=1.0,\n",
    "            value=0.90,\n",
    "            step=0.01,\n",
    "            help=\"Sets the probability threshold for the nucleus sampling algorithm. It controls the diversity of the model's responses.\",\n",
    "        )\n",
    "        st.session_state.num_predict = st.slider(\n",
    "            \"Response Tokens\",\n",
    "            min_value=0,\n",
    "            max_value=8192,\n",
    "            value=2048,\n",
    "            step=16,\n",
    "            help=\"Sets the maximum number of tokens the model can generate in response to a prompt.\",\n",
    "        )\n",
    "\n",
    "        st.markdown(\"---\")\n",
    "        st.text(\n",
    "            f\"\"\"Stats:\n",
    "- model: {st.session_state.model}\n",
    "- seed: {st.session_state.seed}\n",
    "- temperature: {st.session_state.temperature}\n",
    "- top_p: {st.session_state.top_p}\n",
    "- num_predict: {st.session_state.num_predict}\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "\n",
    "def create_chat_model() -> ChatOllama:\n",
    "    return ChatOllama(\n",
    "        model=st.session_state.model,\n",
    "        seed=st.session_state.seed,\n",
    "        temperature=st.session_state.temperature,\n",
    "        top_p=st.session_state.top_p,\n",
    "        num_predict=st.session_state.num_predict,\n",
    "    )\n",
    "\n",
    "\n",
    "def create_chat_chain(chat_model: ChatOllama):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", st.session_state.system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    return prompt | chat_model\n",
    "\n",
    "\n",
    "def update_sidebar_stats(response: Any) -> None:\n",
    "    total_duration = response.response_metadata[\"total_duration\"] / 1e9\n",
    "    st.session_state.total_duration = f\"{total_duration:.2f} s\"\n",
    "    st.session_state.input_tokens = response.usage_metadata[\"input_tokens\"]\n",
    "    st.session_state.output_tokens = response.usage_metadata[\"output_tokens\"]\n",
    "    st.session_state.total_tokens = response.usage_metadata[\"total_tokens\"]\n",
    "    token_per_second = (\n",
    "        response.response_metadata[\"eval_count\"]\n",
    "        / response.response_metadata[\"eval_duration\"]\n",
    "    ) * 1e9\n",
    "    st.session_state.token_per_second = f\"{token_per_second:.2f} tokens/s\"\n",
    "\n",
    "    with st.sidebar:\n",
    "        st.text(\n",
    "            f\"\"\"\n",
    "- input_tokens: {st.session_state.input_tokens}\n",
    "- output_tokens: {st.session_state.output_tokens}\n",
    "- total_tokens: {st.session_state.total_tokens}\n",
    "- total_duration: {st.session_state.total_duration}\n",
    "- token_per_second: {st.session_state.token_per_second}\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    st.set_page_config(page_title=PAGE_TITLE, page_icon=PAGE_ICON, layout=\"wide\")\n",
    "    st.markdown(\n",
    "        \"\"\"\n",
    "        <style>\n",
    "            MainMenu {visibility: hidden;}\n",
    "            footer {visibility: hidden;}\n",
    "            header {visibility: hidden;}\n",
    "        </style>\n",
    "        \"\"\",\n",
    "        unsafe_allow_html=True,\n",
    "    )\n",
    "\n",
    "    st.title(f\"{PAGE_TITLE} {PAGE_ICON}\")\n",
    "\n",
    "    st.markdown(\"##### Chat\")\n",
    "\n",
    "    initialize_session_state()\n",
    "    create_sidebar()\n",
    "\n",
    "    chat_model = create_chat_model()\n",
    "    chain = create_chat_chain(chat_model)\n",
    "\n",
    "    msgs = StreamlitChatMessageHistory(key=\"special_app_key\")\n",
    "    if not msgs.messages:\n",
    "        msgs.add_ai_message(\"How can I help you?\")\n",
    "\n",
    "    chain_with_history = RunnableWithMessageHistory(\n",
    "        chain,\n",
    "        lambda session_id: msgs,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "    )\n",
    "\n",
    "    for msg in msgs.messages:\n",
    "        st.chat_message(msg.type).write(msg.content)\n",
    "\n",
    "    if prompt := st.chat_input(\"Type your message here...\"):\n",
    "        st.chat_message(\"human\").write(prompt)\n",
    "\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            config = {\"configurable\": {\"session_id\": \"any\"}}\n",
    "            response = chain_with_history.invoke({\"input\": prompt}, config)\n",
    "            logger.info({\"input\": prompt}, config)\n",
    "            st.chat_message(\"ai\").write(response.content)\n",
    "            logger.info(response)\n",
    "            update_sidebar_stats(response)\n",
    "\n",
    "    if st.button(\"Clear Chat History\"):\n",
    "        msgs.clear()\n",
    "        st.rerun()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugginface\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Hugging Face has become a go-to platform for accessing and utilizing a vast library of pre-trained machine learning models. From natural language processing (NLP) tasks like text generation and sentiment analysis to computer vision applications like image classification and object detection, Hugging Face offers a user-friendly interface and a diverse collection of cutting-edge models.\n",
    "Hugging Face provides a simple and intuitive API, making it easy to integrate these models into your projects.The platform hosts a wide range of models, from well-known architectures like BERT and GPT to more specialized models for specific tasks.The Hugging Face community is active and supportive, providing valuable resources like tutorials, documentation, and forums. Many models can be fine-tuned on your own data, allowing you to adapt them to your specific needs.\n",
    "To get started, you can create a Hugging Face Account: Sign up for a free account on the Hugging Face platform.\n",
    "\n",
    "**Explore the Model Hub:** Browse the Model Hub to discover models relevant to your needs.\n",
    "\n",
    "**Choose and Load a Model:** Select a model and use the Hugging Face library to load it into your project.\n",
    "\n",
    "\n",
    "**Use the Model:** Utilize the model for your desired task, such as text generation, sentiment analysis, or image classification.\n",
    "\n",
    "**Fine-tune (Optional):** If needed, fine-tune the model on your own data to improve its performance on specific tasks.\n",
    "\n",
    "Hugging Face provides a powerful and accessible platform for leveraging the power of AI. By utilizing their pre-trained models, developers and researchers can quickly and easily integrate advanced machine learning capabilities into their projects, accelerating innovation and driving progress in various fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install transformers\n",
    "#!pip install huggingface_hub\n",
    "#!pip install sentencepiece \n",
    "#!pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "import torch\n",
    "torch.__version__\n",
    "#pip install 'transformers[torch]'\n",
    "#pip install 'transformers[tf-cpu]'\n",
    "#!pip install modelscope\n",
    "#!pip install accelerate>=0.26.0\n",
    "#!pip install jmespath\n",
    "#!pip install  pkginfo  pycryptodomex  texttable  types-PyYAML types-requests\n",
    "#!pip install -U bitsandbytes\n",
    "#!pip install \"transformers>=4.45.1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API_KEY: 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\n"
     ]
    }
   ],
   "source": [
    "#!pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"C:/Users/nboateng/OneDrive - Nice Systems Ltd/Documents/Research/LLM/huggingface_api/.env\")  # take environment variables from .env.\n",
    "\n",
    "# Code of your application, which uses environment variables (e.g. from `os.environ` or\n",
    "# `os.getenv`) as if they came from the actual environment.\n",
    "\n",
    "# Access environment variables as if they came from the actual environment\n",
    "API_KEY = os.getenv('API_KEY')\n",
    "\n",
    "\n",
    "HF_TOKEN  = os.getenv('API_KEY')\n",
    "\n",
    "# Example usage\n",
    "print(f'API_KEY: {API_KEY}')\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token = HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is machine learning?\n",
      "Machine learning is a branch of artificial intelligence that enables computers to learn from data without being explicitly programmed\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "checkpoint = \"HuggingFaceTB/SmolLM-360M\"\n",
    "models  = [\"HuggingFaceTB/SmolLM-1.7B-Instruct\",\"HuggingFaceTB/SmolLM-360M-Instruct\",\"HuggingFaceTB/SmolLM-135M-Instruct\"]\n",
    "#checkpoint = models[1]\n",
    "#checkpoint =    \"nroggendorff/smallama-it\"\n",
    "device = \"cpu\" # for GPU usage or \"cpu\" for CPU usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "prompt = \"what is machine learning?\"\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.42 s\n",
      "Wall time: 5.48 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'user\\nWhat is the capital of the united states of america?\\nassistant\\nThe capital of the United States of America is Washington, D.C.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "models  = [\"HuggingFaceTB/SmolLM-1.7B-Instruct\",\"HuggingFaceTB/SmolLM-360M-Instruct\",\"HuggingFaceTB/SmolLM-135M-Instruct\"]\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(prompt,device,model,top_p,temperature,max_new_tokens):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "  # for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "  model = AutoModelForCausalLM.from_pretrained(model).to(device)\n",
    "  messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "  #input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "  inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "  #inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "  output = model.generate(inputs, temperature=temperature, top_p=top_p, max_new_tokens=max_new_tokens,do_sample=True)\n",
    "  generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "  return generated_text\n",
    "\n",
    "\n",
    "\n",
    "# checkpoint = models[0]\n",
    "\n",
    "# device = \"cpu\" # for GPU usage or \"cpu\" for CPU usage\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# # for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "# model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "# messages = [{\"role\": \"user\", \"content\": \"What is the capital of the united states of america?\"}]\n",
    "# input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "# print(input_text)\n",
    "# inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "# outputs = model.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "# print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "\n",
    "prompt = \"What is the capital of the united states of america?\" \n",
    "device = \"cpu\"\n",
    "model = models[0]\n",
    "temperature=0.2\n",
    "top_p=0.9\n",
    "max_new_tokens= 50\n",
    "generate_text(prompt,device,model,temperature,top_p,max_new_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1ffb153ddd489c95ed3299ef0d5da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nboateng\\.ai-navigator\\conda\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nboateng\\.cache\\huggingface\\hub\\models--HuggingFaceTB--SmolLM-1.7B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fb078a073847da91ca6d744e501fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6e05e6345f4690a4dd1a5f30ca1f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93da3bb0558442f09c9ac25e3abff500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503bc6b85eb24526b15d0a54ab0ebd5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c0c1cb0f54436b893834e868f05029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/738 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6eeca9e23ab4f879a057a624540962d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9f9b8f06594483b695bf43139ad24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "What is the capital of the united states of america?<|im_end|>\n",
      "\n",
      "<|im_start|>user\n",
      "What is the capital of the united states of america?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The capital of the United States of America is Washington, D.C.<|im_end|>\n",
      "CPU times: total: 15.2 s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# pip install transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "models  = [\"HuggingFaceTB/SmolLM-1.7B-Instruct\",\"HuggingFaceTB/SmolLM-360M-Instruct\",\"HuggingFaceTB/SmolLM-135M-Instruct\"]\n",
    "\n",
    "checkpoint = models[0]\n",
    "\n",
    "device = \"cpu\" # for GPU usage or \"cpu\" for CPU usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is the capital of the united states of america?\"}]\n",
    "input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(input_text)\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fastchat-t5-3b-v1.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "HF_MODEL = 'lmsys/fastchat-t5-3b-v1.0'\n",
    "#HF_MODEL = 'Genius-Society/gpt4all'\n",
    "#model_id =  \"SweatyCrayfish/llama-3-8b-quantized\"\n",
    "HF_MODEL_PATH = HF_MODEL.split('/')[1]\n",
    "HF_MODEL_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT4All\n",
    "\n",
    "https://www.nomic.ai/gpt4all\n",
    "\n",
    "Nomic.ai's GPT4All is an open-source desktop application allows you to run LLMs directly on your device, without the need for cloud access or expensive hardware. With GPT4All, you can harness the power of LLMs for a wide range of tasks, including:\n",
    "\n",
    "**Writing:** GPT4All can help you brainstorm ideas, overcome writer's block, and even generate different creative text formats of text content, like poems, code, scripts, musical pieces, email, letters, etc.\n",
    "\n",
    "**Research:** Use GPT4All to summarize complex topics, translate research papers, and find relevant information from your local documents.\n",
    "\n",
    "**Coding:** GPT4All can assist you with coding tasks by generating code snippets, debugging code, and writing documentation.\n",
    "\n",
    "**Learning:** GPT4All can be a valuable tool for learning new things. Ask it questions, get explanations of complex concepts, and even have it generate practice problems.\n",
    "\n",
    "\n",
    "Getting started with GPT4All is easy. Simply download the application from the Nomic.ai website and install it on your device. Once installed, you can start using GPT4All to explore the power of LLMs.\n",
    "\n",
    "GPT4All is a powerful tool that is democratizing access to LLMs. With its continued development, GPT4All is poised to play a major role in the future of artificial intelligence. The recent update of GPT4All v3.4.0, which includes faster models and expanded filetype support. GPT4All is not only for individuals but can also be used by businesses for enterprise purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LM Studio\n",
    "\n",
    "https://lmstudio.ai/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LM Studio is a powerful application that allows you to run large language models (LLMs) directly on your computer. This means you can harness the power of these sophisticated AI models for a variety of tasks, all without relying on cloud services or powerful graphics cards. Developed by a team of AI researchers at Microsoft, LmStudio uses machine learning algorithms to generate natural-sounding text based on your input.\n",
    "\n",
    "**Key Features of LM Studio**\n",
    "\n",
    "**Local Processing:** Run LLMs on your own hardware, keeping your data private and secure. By running LLMs locally, you can ensure that your data never leaves your device, providing greater privacy and security. LM Studio can even be used  when you don't have an internet connection, making it a valuable tool for situations where connectivity is limited.\n",
    "\n",
    "**Wide Model Support:** LM Studio supports a variety of LLM models, including Llama 3.2, Mistral, Phi, Gemma, and DeepSeek. This gives you the flexibility to choose the model that best suits your needs.\n",
    "\n",
    "**Chat Interface:** Interact with LLMs through a user-friendly chat interface, allowing you to have conversations and ask questions in a natural way.\n",
    "\n",
    "**OpenAI Compatibility:**  LM Studio's local server is compatible with OpenAI API, making it easy to integrate with existing tools and workflows.\n",
    "\n",
    "**Hugging Face Integration:** Download compatible model files directly from the Hugging Face model repository, giving you access to a vast collection of pre-trained models.\n",
    "\n",
    "**Integration with Other Tools:** LM Studio's local server can be integrated with other tools, such as text editors, chatbots, and AI development platforms, making it easy to collaborate and build custom solutions.\n",
    "\n",
    "**Community Support:** Join the LM Studio community on Discord to ask questions, share resources, and collaborate with other users.\n",
    "\n",
    "**Cost and Availability:** LM Studio is free to use for personal purposes, making it an accessible option for individuals and hobbyists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: resource in c:\\users\\nboateng\\.ai-navigator\\conda\\lib\\site-packages (0.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement signal (from versions: none)\n",
      "ERROR: No matching distribution found for signal\n"
     ]
    }
   ],
   "source": [
    "!pip install resource "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'resource'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLM, SamplingParams\n\u001b[0;32m      3\u001b[0m prompt \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, my name is\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe capital of united states is\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#llm = LLM(model=\"meta-llama/Llama-2-70b-chat-hf\")\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"vLLM: a high-throughput and memory-efficient inference engine for LLMs\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marg_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncEngineArgs, EngineArgs\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masync_llm_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsyncLLMEngine\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_engine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMEngine\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\engine\\arg_utils.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01menvs\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (CacheConfig, CompilationConfig, ConfigFormat,\n\u001b[0;32m     12\u001b[0m                          DecodingConfig, DeviceConfig, HfOverrides,\n\u001b[0;32m     13\u001b[0m                          KVTransferConfig, LoadConfig, LoadFormat, LoRAConfig,\n\u001b[0;32m     14\u001b[0m                          ModelConfig, ObservabilityConfig, ParallelConfig,\n\u001b[0;32m     15\u001b[0m                          PoolerConfig, PromptAdapterConfig, SchedulerConfig,\n\u001b[0;32m     16\u001b[0m                          SpeculativeConfig, TaskOption, TokenizerPoolConfig,\n\u001b[0;32m     17\u001b[0m                          VllmConfig)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutor_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExecutorBase\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\config.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompilation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minductor_pass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CallableInductorPass, InductorPass\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (QUANTIZATION_METHODS,\n\u001b[0;32m     23\u001b[0m                                                      get_quantization_config)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelRegistry\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m current_platform, interface\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\model_executor\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (BasevLLMParameter,\n\u001b[0;32m      2\u001b[0m                                            PackedvLLMParameter)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msampling_metadata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (SamplingMetadata,\n\u001b[0;32m      4\u001b[0m                                                    SamplingMetadataCache)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_executor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_random_seed\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\model_executor\\parameter.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parameter\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tensor_model_parallel_rank\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[0;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBasevLLMParameter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackedvLLMParameter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerTensorScaleParameter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelWeightParameter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChannelQuantScaleParameter\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroupQuantScaleParameter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPackedColumnParameter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRowvLLMParameter\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\distributed\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommunication_op\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel_state\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\distributed\\communication_op.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel_state\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tp_group\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtensor_model_parallel_all_reduce\u001b[39m(input_: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"All-reduce the input tensor across model parallel group.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\distributed\\parallel_state.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Backend, ProcessGroup\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkv_transfer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkv_transfer_agent\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkv_transfer\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01menvs\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StatelessProcessGroup\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\distributed\\kv_transfer\\kv_transfer_agent.py:15\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VllmConfig\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkv_transfer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkv_connector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfactory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     KVConnectorFactory)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntermediateTensors\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\distributed\\kv_transfer\\kv_connector\\factory.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KVConnectorBase\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VllmConfig\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\distributed\\kv_transfer\\kv_connector\\base.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, List, Tuple, Union\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntermediateTensors\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VllmConfig\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\sequence.py:16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmsgspec\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SingletonInputs, SingletonInputsAdapter\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlora\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoRARequest\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultimodal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiModalDataDict, MultiModalPlaceholderDict\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\inputs\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (DecoderOnlyInputs, EncoderDecoderInputs,\n\u001b[0;32m      2\u001b[0m                    ExplicitEncoderDecoderPrompt, ProcessorInputs, PromptType,\n\u001b[0;32m      3\u001b[0m                    SingletonInputs, SingletonInputsAdapter, SingletonPrompt,\n\u001b[0;32m      4\u001b[0m                    TextPrompt, TokenInputs, TokensPrompt,\n\u001b[0;32m      5\u001b[0m                    build_explicit_enc_dec_prompt, to_enc_dec_tuple_list,\n\u001b[0;32m      6\u001b[0m                    token_inputs, zip_enc_dec_prompts)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (DummyData, InputContext, InputProcessingContext,\n\u001b[0;32m      8\u001b[0m                        InputRegistry)\n\u001b[0;32m     10\u001b[0m INPUT_REGISTRY \u001b[38;5;241m=\u001b[39m InputRegistry()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03mThe global :class:`~InputRegistry` which is used by :class:`~vllm.LLMEngine`\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03mto dispatch data processing according to the target model.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    :ref:`input-processing-pipeline`\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\inputs\\registry.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_logger\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cached_get_processor\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnyTokenizer\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ClassRegistry, get_allowed_kwarg_only_overrides,\n\u001b[0;32m     15\u001b[0m                         print_warning_once, resolve_mm_processor_kwargs)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProcessorInputs, SingletonInputs\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\transformers_utils\\tokenizer.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MistralTokenizer\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_gguf_file\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_async\n\u001b[0;32m     18\u001b[0m logger \u001b[38;5;241m=\u001b[39m init_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     20\u001b[0m AnyTokenizer \u001b[38;5;241m=\u001b[39m Union[PreTrainedTokenizer, PreTrainedTokenizerFast,\n\u001b[0;32m     21\u001b[0m                      MistralTokenizer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vllm\\utils.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mresource\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msignal\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msocket\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'resource'"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "prompt = [\"Hello, my name is\", \"The capital of united states is\"]\n",
    "\n",
    "#llm = LLM(model=\"meta-llama/Llama-2-70b-chat-hf\")\n",
    "llm = LLM(model=\"gpt2\")\n",
    "sampling_params = SamplingParams(temperature=0.5)\n",
    "outputs = llm.generate(prompt, sampling_params=sampling_params)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/legraphista/DeepSeek-V2-Lite-Chat-IMat-GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JellyBox\n",
    "\n",
    "https://jellybox.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local AI\n",
    "\n",
    "https://localai.io/\n",
    "\n",
    "\n",
    "LocalAI empowers you to harness the power of advanced AI models right on your own computer. It functions as a direct replacement for the OpenAI API, meaning you can use it with existing applications designed for OpenAI. You can run LLMs, generate images, audio etc. locally or on-prem with consumer grade hardware, supporting multiple model families and architectures without a GPU hardware. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
